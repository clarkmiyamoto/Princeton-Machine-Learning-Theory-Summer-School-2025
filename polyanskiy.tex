\section{Motivation}
Information theory was motivated by attempting to study how much "information" is lost when we move from the continuous time signals to bits. In LLM, you end up just computing matrix multiplication. However keeping the entries of the matrix at full precision (i.e. float32), this is expensive (both in storage, and in transfer time between GPU and CPU), so what happens when we move from $\text{float}32 \to \text{float}4$? This is \textbf{quantization}.
\\
\\
In a modern context, you have a transformer which converts text to tokens (integers), pictures to soft-tokens (Polyanskiy says this is a vector in $\mathbb R^{d_{model}}$). \\
\\
At the end of these lectures we'll prove a theorem on the optimal compression rate for quantizing the multiplication of two Gaussian iid matrices.
$$
\mathbb E || \hat Y - Y||_F^2
$$
where $Y = A B$, and $\hat Y$ is the same product where $A,B$ have been quantized.

\section{Scalar Quantization (Information Theory)}
\subsection{Uniform Quantization}
Consider a quantization $x \in \mathbb R$, and I want to construct a map $x \mapsto \epsilon \in [x / \epsilon]$, where $[n] = [1,...,n]$. This is called \textbf{uniform quantization (UQ)} (in machine learning this is called \textbf{round to nearest}). We can then ask, what is the error of UQ?

Consider $x \sim p_x$ (which is $U[-1/2, 1/2]$). This means as you take $\epsilon \to 0$ you find
\begin{align}
	x | [x/\epsilon] = m] \approx \text{Uniform}[-\epsilon /2, \epsilon/2]
\end{align}
Then you can compute
\begin{align}
	\mathbb E[(x -q(x))^2]  = \frac{\epsilon^2}{12} (1 + \mathcal O(1))=  \frac{2^{-2R}}{12}
\end{align}

\subsection{Dithering}
Is there a rigorous way to write $\mathbb E[(x-q_\epsilon(x))^2] = \epsilon^2/12$?

To do this, we introduce this cool math trick. Let $U \sim \text{Uniform}[-\epsilon/2, \epsilon/2] \perp X$. Now consider $Y = X+ U$. Now quantize $Y$, which is $q(Y) = q(X+U) = \epsilon[(x+u)/2]$. Now you subtract off the $U$, so your quantized estimate of $X$ is
\begin{align}
	\hat X = q(X+U) - U
\end{align}

\begin{proposition}
	[Crypto Lemma] $(\hat X, X) =^{(d)} (X+ \tilde U, X)$, where $X \perp \tilde U \sim \text{Uniform}[-\epsilon/2,\epsilon,2]$
	\\
	\\
	Proof: Notice that
	\begin{align}
		\hat X - X = q(X +U ) - (X+U) = \text{Uniform}[-\epsilon/2, \epsilon/2] \perp X
	\end{align}
	Note that this holds for any discretization / lattice scheme.
\end{proposition}
What's really nice is that this makes any resulting calculation of $f(X)$ very simple. I.e. consider you're wanting to see $Y = X^2$
\begin{align}
	\mathbb E[(Y - \hat Y)^2] = \mathbb E[(X^2 - \hat X^2)^2] = \mathbb E[(X^2 - (X+U)^2)^2] = \frac{\epsilon^2}{3}\mathbb E[X^2] + \text{constant}
\end{align}

\subsection{What about non-uniform $q$?}
OpenAI's newest's open source model GPT-OSS uses MX-FP4 quantization. Your quantization domain becomes $\{0, \pm 1/2, \pm 1, \pm 3/2, \pm 2, \pm 4, \pm 6 \}$. They got these numbers because it's useful for hardware.


\begin{definition}
	For a given $p_x$. The best quantization $D_{scalar}$ is defined as
	\begin{align}
		D_{scalar} (R, P_x) & = \inf_{|\text{image } q| \leq 2^R} \mathbb E[|X-q(X)|^2]\\
		& = \inf_{p_{x,y}} \{ \mathbb E[|X-Y|^2] : |\sup p_Y| \leq 2^R\}
	\end{align}
	So this is secretly the Wasserstein-2 projection. And it's very non-convex :(
\end{definition}


Fix $\text{im}(q) = \{c_1 <  ... < c_N\} = G$. The optimal quantization of $q$ is the nearest $n$-bin $C = \{B_j\}_j$. 

Now that you're given the bins $\{B_j\}_{j=1}^N$, the optimal quantization $q(x_ = \mathbb E[X | X \in B_j\}$.


\subsection{What is the best non-uniform quantization?}
\begin{theorem}
	[Panter-Diter '51] Under mild assumptions
	\begin{align}
		D_{scalar}(R; p_x) = \frac{2^{-2R}}{12} \left ( \int p_x(x)^{1/3} dx\right)^3
	\end{align}
	\\
	\\
	Proof: Let $R \to \infty$ and $N \to \infty$. So this means the number of points in interval I $:= N \int_I \lambda(x) dx$ So now you need to ask
	\begin{align}
		\mathbb E[(x-q(x))^2] & = \sum \mathbb E[(x-c_j)^2] \mathds 1_{x \in I_j}\\
		&  \approx \sum \frac{\epsilon_j^2}{12} \mathbb P[x \in I_j]\\
		& \approx \frac{1}{12 N^2} \int \lambda^{-2}(x) f(x) dx \\
		& \geq C  \left(\int f^{-1/3} \right)^3 
	\end{align}
	 QED
\end{theorem}
There is some heuristic for doing this on a gaussian... But I missed it lol.




\section{Vector Quantization}

From last time, we did $X \mapsto \hat X \in \{\text{Discrete subset of } \mathbb Z^R\}$, and we found that $\lambda^* \sim \text{pdf}^{1/3}$.


Now let's do vector quantization. Let $X = [x_1, ..., x_n]$, and perform the quantization map $\hat X = [\lfloor x_1 \rfloor, ..., \lfloor x_n \rfloor]$. So obviously, we're interested in determining the optimal quantization, but it is always optimal w.r.t. a particular operation. In this case, Polyanskiy says you're interested in dot products
\begin{align}
	\min_{\hat X} \mathbb E[(A^T X - A^T \hat X)^2] = \min_{\hat X} \mathbb E[(A^T (X - \hat X))^2] = (X - \hat X) H (X - \hat X)
\end{align}
where $H = \mathbb E[AA^T]$.\\
\\
\begin{sidework}
An observation:

It is impossible to solve this (post-quantum) [He doesn't know what post quantum means, but heuristically this problems depends on nearest-neighbor lattice search, and quantum computers have a hard time with sparse graphs]. Here's a proof that this is a nearest-neighbor lattice search.

	Notice that $H$ is a linear transformation, so by 3Blue1Brown, you're skewing the coordinate space that the vectors $X,\hat X$ live on. If you quantize the coordinate space, you'll notice you're on a lattice, and figuring out where to optimally place $\hat X$ on said lattice it's a lattice search problem. soft-QED.
\end{sidework}

\begin{sidework}
	Boris asks a question: why not just optimize on $\min_{\hat X \in Q \mathbb Z^R}$ (that is instead of quantizing on the skewed lattice, you quantize on the square lattice, and  forget about $Q$)? This doesn't work because you eventually will need to communicate $Q$, which costs bits, hence you're work will be lost forever :(
\end{sidework}
\textbf{Goal:} You have $X \in \mathbb R^n$, and we want a map $X \mapsto \hat X = q(x)$ where $|\text{image}(q)| = 2^{nR}$.
\begin{align}
	q = \sum_j c_j \mathds  1_{\mathcal D_j}
\end{align}

A particular solution is the \textbf{Lloyd Max algorithm}
\begin{enumerate}
	\item For fixed $\mathcal C = \{c_1, ..., c_N\}$, the optimal $q$ is
		\begin{align}
			q(x) = \text{argmin}_{c \in \mathcal C}  ||x - c||
		\end{align}
	\item For fixed $\mathcal D_1 ,..., \mathcal D_n$, the optimal 
		\begin{align}
			c_j = \mathbb E[X | X\in \mathcal D_j]
		\end{align}
\end{enumerate}


\subsection{VQ-VAE}
An application of vector quantization in machine learning is VQ-VAE (Vector quantization - variational auto encoder). You have the following autoencoder structure

\begin{align}
	f_\theta(X) = Z = (z_1,..., z_m)\\
q_c (Z)  = \hat Z = (\hat z_1, ..., \hat z_m)\\
g_\varphi(\hat Z) = \hat X
\end{align}
so your end to end network is $\hat X = g_\varphi(q_c(f_\theta(X)))$. Your objective is reconstruction, so your loss function is
\begin{align}
	\mathbb E_x||X -\hat X||^2_2
\end{align}
however the problem is if you differentiate this loss, the gradients won't flow to $\theta$, as $q_c$ is a step function, so certain points won't generate any gradient. The trick to get around this is to turn-off the $q_c$ block on backprop.
\begin{align}
	||X - g_\varphi(\hat Z) ||^2_2 + \lambda \sum_i || z_i - \hat z_i ||_2^2
\end{align}
People also use this trick in mixture of experts models to allow gradients to flow through.\\
\\
Polyanskiy also has an aside on analytically computing the gradients on the first loss, and proposes an alterative solution (instead of using the 2nd loss).

\subsection{Information Theory}
Let $\underline x = (x_1 , ..., x_n) \sim p_x$. Find the quantizer $q(\underline x)$ with $| \text{image} (q)| = 2^{nR}$ s.t.
\begin{align}
	\min \mathbb E[||\underline x - q(\underline x)||^2]
\end{align}

\textbf{Property:}
For all $q$, there exists a coupling $p_{x,y} \in \mathcal P(\mathbb R \times \mathbb R)$ s.t.
\begin{enumerate}
	\item $I(x; y) \leq R$
	
	\emph{Proof:} 
	\begin{align}
		R & \geq \frac{1}{n} I(\underline x ; \hat{ \underline x}) \\
		& = \frac{1}{n} (S(x_1^n) - S(x_1^n | \hat x_1^n)\\
		& = \sum_i S(x_i) - S(x_i | \hat x_1^n, x^{i-1}_1) \leq ... - S(x_i | \hat x_i^n)\\
		& \geq \frac{1}{n} \sum_i S(x_i) - S(x_i, \hat x_i) = \frac{1}{n} \sum_i I(x_i ; \hat x_1)
	\end{align}
	where $S(\cdot)$ is entropy, and $S(\cdot | *)$ is the conditional entropy. If $x_1^n, \hat x_1^n \perp \tau \sim \text{Uniform}([n])$, then we notice that 
	\begin{align}
		= I(x; y | \tau)
	\end{align}
	and now $\tau \perp x$, you find that $I(x; y | \tau) = I(x; y, \tau)  \geq I(x;y)$. \emph{QED.}
	\item $\mathbb E[(x-y)^2] \leq \frac{1}{n} \mathbb E_x [||x - q(x)||^2]$
	
	\emph{Proof:} $\mathbb E[(x-y)^2] = \frac{1}{n} \sum_i \mathbb E[(x_i - \hat x_i)^2] = \frac{1}{n} ||x-q(x)||^2 $ \emph{QED.}
\end{enumerate}

\begin{corollary}
	$\forall q$ we must have $\mathbb E[||\underline x - q(\underline x)||^2] \geq n  D_{vec}(R)$ where
	\begin{align}
		D_{vec}(R)=  \inf_{p_{y|x}} \{ \mathbb E[(x-y)^2] : I(x;y) \leq R\}
	\end{align} If you're an optimal person (I am not), you'll notice that this is an EOT projection, hence it's a convex problem.\\
	\\
	\emph{Proof:} Take the lagrange dual of formulation of $D_{vec}(R)$. This yields
	\begin{align}
		D_{vec}(R) = \min \mathbb E( x - y )^2 + \lambda I(x; y)
	\end{align}
\end{corollary}\begin{sidework}
	Example:
	
	Consider $p_x = \mathcal N(0,1)$. If $x \sim \mathcal N$, then $\forall Y$ (which is a random variable) if we make $Y_G$ be the Gaussian version of $Y$ (that $Y_G \sim \mathcal N(\mu_y, \sigma_y^2)$, is set s.t. $\mu_y = \mathbb E[Y], \sigma_y^2 = \text{Var}(Y)$). One can show
	\begin{align}
		I(\underline X; \underline Y) \geq I (\underline X; \underline Y_G)
	\end{align}
	But isn't this trivial? This is a non-linear transformation of the distribution, hence it's strictly decreasing...
	
	Anyways... Let's consider $X = \alpha Y + \beta Z$ , where $X,Y \sim \mathcal N(0, \Sigma)$ and $Z \sim \mathcal N(0,1)$. This implies
	\begin{align}
		I(X;  Y) = \frac{1}{2} \log ( 1 + \mathbb E(\alpha Y)^2  / \mathbb E(\beta Z)^2) = \frac{1}{2} \log (1 + \alpha^2 \sigma^2 / \beta^2\\
		\mathbb E(X-Y)^2 = (1- \alpha)^2 \sigma^2 + \beta^2 
	\end{align}
	These implies $\alpha = 1$.\\
	\\
	Now you redo it
	\begin{align}
		D = \beta^2\\
		I = \frac{1}{2} \log (1/\beta^2)
	\end{align}
	This implies $D_vec = 2^{-2R}$
\end{sidework}

\begin{theorem}
	For all $p_{y | x}$ with $I(X; Y) \leq R$, there exists $q$ such that $|q(x)| \leq 2^{nR}$. That is $\mathbb E||x - q(x)||^2 \leq n \mathbb E(x-y)^2 + \mathcal O(n)$
\end{theorem}
\begin{theorem} [Generalization of last theorem]
	Consider (\underline x, \underline y) on $\mathbb R^n \times \mathbb R^n$. For all $M, \gamma$, there exists $q: \underline x \to \{c_1, ..., c_{n+1}\}$ such that
	\begin{align}
		\mathbb E[||\underline x - q(\underline x)||^2] \leq \mathbb E[||\underline x - \underline y||^2] + e^{-M/\gamma}\mathbb E [||\underline x||^2] + \mathbb E||\underline x||^2 \mathds 1\{i(\underline x; \underline y) > \log \gamma\}
	\end{align}
	where $i (\underline a; \underline b) = \log p_{x,y}(a,b) / p_x(a) p_y(b)$ is the information density.
\end{theorem}
To see what the RHS of this bound means, let's see an application.

Let $x = (x_1,..., x_n) \sim p_x$ and $y := (y_1, ..., y_n) \sim p_{y|x}$. Where the coordinates are independent of each other.
\begin{align}
	i(a^n ; b^n) \equiv \log \frac{p_{x,y}}{p_{x} p_y} = \sum_{t=1}^n
 \log \frac{p_{x,y}}{p_x p_y}\Big|_{a_t, b_t}
 \end{align} 
 This means in the limit as $n \gg 1$
 \begin{align}
 	\frac{1}{n} i(x; y) \to \mathbb E [i] = I(x; Y)
 \end{align}


\subsection{Dot Products}
Ok let's go back to dot products. Our objective is to
\begin{align}
	\min \mathbb E[(x^T A - \hat x^T A)^2] \iff \min (x-\hat x)^T H (x - \hat x)
\end{align}
Let's assume $\underline x \sim \mathcal N(0, \mathbb I)$, so your minimization becomes
\begin{align}
	\min \{ I(\underline x; \underline y) : \mathbb E(\underline x - \underline y)^T H (\underline x - \underline y ) \leq D\}
\end{align}
Now we diagonalize our $H = U^T \Lambda U$, and this just is rotation your Gaussian's covariance matrix. From the previous example, we learned that you can just do $\underline y \sim \mathcal N$,
\begin{align}
	\min \{\sum_i I(x_i, y_i) : \sum(x_i -y_i )^2 \beta_i^2 \leq D \}\\
	\min_{\beta_i^2 \in [0,1]} \{ \frac{1}{2} \sum_i \log \frac{1}{\beta^2_i} : \sum_i \beta_i^2 c_i^2 \leq D\}
\end{align}
You can now solve this using Lagrange multipliers.  Now you find that
\begin{align}
	\beta_i^2 \sigma_i^2 = \begin{cases}
		T, & \sigma_i^2 > T\\
		\sigma_i^2 , &\sigma^2_i < T
	\end{cases}
\end{align}
This implies 
\begin{align}
	R_i = \begin{cases}
		\frac{1}{2} \log \frac{T}{\sigma_i^2}, & \sigma_i^2 > T \\ 0, &\sigma^2_i < T
	\end{cases}
\end{align}




\subsection{FSQ}










\subsection{Information Theoretic Quantization}





































\section{VQI (Vector Quantization Information)}
\section{Quantization for LLMs}

\section{Open Problems}