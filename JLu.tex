In these set of lectures, we'll go over how to design machine learning algorithms to solve problems arising from PDEs.

\section{August 12th (1st course)}
Consider a PDE defined on the domain $\Omega \subset \mathbb R^d$. For example let's consider the driven heat equation.
\begin{align}
	-\Delta u &= f & \text{Inside } \Omega\\
	\partial_n u  &= 0 & \text{On } \partial \Omega
\end{align}
where $\Delta = \frac{d}{2} = - \partial^2$ is the laplacian.

We can massage this into a minimization problem, where you attempt to minimize the residual. This is called the \textbf{strong form}
\begin{align}
	\inf_u \int_\Omega |\Delta u + f|^2 
\end{align}
This technique has origins in mathematics long ago, however today, we ansatz $u=u_\theta$ using a neural network, and modify $\theta$ s.t. you minimize that quantity. These are known as \textbf{PINNs (Physics Informed Neural Networks)}.

Another method is to use the \textbf{variational form}
\begin{align}
	\inf_u \frac{1}{2} \int (|\nabla u|^2 - u f) ~dx 
\end{align}
You can convince yourself that this is correct by taking the variational derivative w.r.t. $u$, and see the minimizers is of the form of the PDE.

Another method is to use the \textbf{weak form}, where we expect this
\begin{align}
	\forall v~ \int v (-\Delta u \cdot f) = 0 \iff \int_\Omega \nabla v \cdot \nabla u - v f = 0
\end{align}

\begin{problem}
	To demonstrate your understanding, show the formulation of the strong, variational, and weak form in linear algebra. I.e. you want to solve $A x = b$, reformulate this into the various forms. 
\end{problem}

When realizing these problem on a computers, $u$ is a high-dimensional (or infinite dimensional) quantity, obviously this becomes a problem...

\subsection{Finite Dimensional Approximation (In Function Space)}
Consider decomposing $u$ into a basis $\{\varphi_i\}_i$
\begin{align}
	u(x) = \sum_{i=1}^N c_i \varphi_i(x)
\end{align}
Now it is your job to find $c_i$'s s.t. you minimize the variational form. So let's plug it in
\begin{align}
	\min_{\{c_i\}} \left[\frac{1}{2} \sum_{ij} \left( c_i c_j \int \nabla \varphi_i \cdot \nabla \varphi_j  \right)- \sum_i \left( c_i \int f \varphi_i dx \right) \right]
\end{align}
This is now just a quadratic minimization problem. Nice.\\
\\
However, this problem suffers from the curse of dimensionality. The number of coefficients grows exponential in the dimension. To see this, notice when you descrteize the domain $\Omega$, the number of grid points grows as $\text{(\# points)} = (L/\epsilon)^d$.\\
\\
So solving PDEs in high-dimension (in domain space) is an interesting problems. And it seems that neural networks can defeat the curse of dimensionality, so perhaps we need to combine the two!

\subsection{What can Neural Networks do for PDEs?}
So now that we've motived that neural networks can help us in PDEs, let's go over a few applications
\begin{enumerate}
	\item PINN: Using neural networks ansatz for solving PDEs. We've outlined this problem above.
	\item Operator learning: Your use the neural networks to map a function to another function (thus it learns an operator). I.e. this can be a time-evolution kernel. The difficulty lies in encoding an infinite dimensional space into a finite dimensional space.
	\item System identification:
\end{enumerate}


\subsection{Bounding of Error Due to Discretization}
Let $\mathcal X$ be some space over functions. Your true function $u_* \in \mathcal X$. Now consider a finite-dimensional approximation of the function space $u_N \in \mathcal X_N$, where we assume $\mathcal X_N \subset \mathcal X$ (this may not hold depending on the boundary condition you set). Note that your true solution $u_*$, may not necessarily be in $\mathcal X_N$. This yields a trivial bound on the residual
\begin{align}
	||u_* - u_N||_{x} \geq \underbrace{\inf_{u \in \mathcal X_n} ||u_* - u||_x}_{\text{Approximation error}}
\end{align}
Going back to the forced heat equation in the variational form, we want to minimze
\begin{align}
	\Sigma(u) & = \frac{1}{2} \int |\nabla u |^2 dx - \int u f dx\\
	\inf_{u \in \mathcal X} \Sigma(u) = \Sigma(u_*)
\end{align}
From the bound, we know that $\Sigma(u_N)  - \Sigma(u_*)\geq  0$. Plugging this in
\begin{align}
	\Sigma(u_N) - \Sigma(u_*) & = \frac{1}{2} \int( |\nabla u_N|^2 - |\nabla u_*|^2 ) dx + \int (u_N - u_*) \Delta u_* dx\\
	& = \frac{1}{2} \int |\nabla u_N - \nabla u_*|^2 dx
\end{align}
If $\Omega$ is bounded, then
\begin{align}
	\underbrace{\int |u_N - u_*|^2}_{||u_* - u_N||^2_x} dx < C \underbrace{\int |\nabla u_N - \nabla u_*|^2 dx}_{\sim (\Sigma(u_N) - \Sigma(u_*))}
\end{align}
And if you make an additional assumption that $\Omega$ is bounded AND convex, then you know that $C$ doesn't depend on $\text{dim}(\Omega)$. This gives us a pretty bound
\begin{align}
	\Sigma(u_N) - \Sigma(u_*) \geq  C || u_* - u_N||^2_x
\end{align}\
You can find another bound
\begin{align}
	\Sigma(u_N) - \Sigma(u_*) & = \inf_{u \in \mathcal X_N} \Sigma(u) - \Sigma(u_*)\\
	& = \inf_{u \in \mathcal X_n} \frac{1}{2} \int |\nabla u - \nabla u_*|^2\\
	& \leq \inf_{u \in \mathcal X_N} \frac{1}{2} || u - u_* ||^2_x
\end{align}
Putting this together, you get a very nice principled bound on the variational formulation of the problem.
\begin{align}
	\boxed{C || u_* - u_N||^2_x \leq \Sigma(u_N) - \Sigma(u_*) \leq \inf_{u \in \mathcal X_N} \frac{1}{2} || u_* - u ||^2_x}
\end{align}


\section{August 12th (2nd course)}
Recall that given a PDE, we have some energy functional $\Sigma(u)$ which has a minimizer $u_* \in \mathcal X$. We then ask the question, given a discretization of the function space $\text{argmin}_{u \in \mathcal X_N} \Sigma(u) = u_N \in \mathcal X_N \subset \mathcal X$, how does your approximated $u_N$ compare to the true answer $u_*$
\begin{align}
	\inf_{u \in \mathcal X_N}||u-u_*||_X
\end{align}
Consider the set of functions $H^k(\Omega) = \{u : \int |\partial_{i_1} ... \partial_{i_d} u|^2 d x < \infty, \sum_j i_j = i, i \leq k\}$

\begin{theorem}
	[Bramble-Hilbert Lemma] Let $u$ be a function defined on the domain $\Omega$, and $v \in P_k(\Omega)$ ($k$'th order polynomials defined on domain $\Omega$). Under "mild" assumptions...
	\begin{align}
		\inf_{v \in P_k(\Omega)} \int_\Omega |u^{(i)} - v^{(j)}|^2 dx \leq C (\text{diameter}(\Omega))^{m-k} \int_\Omega |u^{(m)}|^2 dx
	\end{align}
	where $u^{(i)}$ is the $i$'th order derivative, $(m)$ is the highest finite order derivative.\\
	\\
	As a result (which is not apparent to me a priori), the number of degrees of freedom scales $\sim h^{-d/s}$, where $h$ is the mesh size, $d$ is the dimension of the domain space $\Omega$, and $s$ are regularity conditions. So reading into it--- higher dimensional PDEs end up having "simpler" curves.
\end{theorem}

\begin{theorem}
	[Barron 1993] Consider the Barron function space 
	\begin{align}
		B(\mathbb R^d) = \Big\{u:  \mathbb R^d \to \mathbb R \Big | \int_{\mathbb R^d} |\omega | | \hat u (\omega) | d\omega < \infty\Big\}
	\end{align}
	where $\hat u$ is the Fourier transform of $u$. 
	\begin{align}
		H^1(\mathbb R^d) & = \{u : \mathbb R^d \to \mathbb R | \int |u|^2 + |\nabla u|^2 dx < \infty\}\\
		& = ... \int |\hat u (\omega)|^2 + |\omega|^2 \hat u (\omega)|^2 < \infty
	\end{align}
	The 2nd equality holds by Parseval's theorem, which states the $L_2$ norm is invariant under FT Transform.
	\\
	\\
	Then $\forall N$, there exists 
	\begin{align}
		u_N(x) = \sum_{i=1}^N a_i \sigma(b_i x + c_i)
	\end{align} s.t. the approximation error is bounded
	\begin{align}
		\int |u - u_N|^2 \mu(dx) \leq C \frac{||u||^2_{B(\mathbb R^d)}}{N}
	\end{align}
	\begin{sidework}
		To me this is kinda surprising because $N$ is the number of basis function which you use to approximate $u$. You'd imagine that as you increase the number of basis functions that such functions "over fit", and start to have high residuals... But I might be thinking of this wrong.
	\end{sidework}	
\end{theorem}

\begin{lemma}
	Let $f \in$ convex hull of a set $C$ in $X$ (Hilber space), with $||g|| \leq b$. You find that $\forall g \in G$, then for any $n \geq 1$ every $c > b^2 - ||f||^2$ there exists lies in the convex hull of $G$, s.t.
	\begin{align}
		||f-f_n||^2 \leq \frac{c}{n}
	\end{align}
	Proof: Let $||f - f^*|| \leq \delta / n$ where $f^*$ in the convex hull of $G$. Now do a basis decomposition
	\begin{align}
		f^*= \sum_{i=1}^M \nu_i g_{il}^*, ~~~~~\text{s.t.}~ \gamma_i \in [0,1], \sum_i \gamma_i =1
	\end{align}Randomly sample $g_1,..., g_n \sim \{g_j\}j$ with probability. This means 
	\begin{align}
		f_n= \frac{1}{n} \sum_{i}g_i
	\end{align}
\end{lemma}

Ok let's consider a screened poisson equation
\begin{align}
	(-\Delta + \gamma^2) u & = f & \text{Defined on } \mathbb R^d
\end{align}
Just by inspection, if $f$ is defined to the $k$ derivative, then we'd imagine that $u$ should be well defined for the $k+2$ derivatives. The LHS has two more derivatives. This implies $||u||_{H^{k+2}(\mathbb R^d)} \leq ||f||_{H^k(\mathbb R^d)}$.

\subsection{Cousins of Barron Spaces}
\subsubsection{Compact Spaces}
Consider a domain $\Omega = [0,1]^d$, you can define a barron space 
\begin{align}
	B^k([0,1]^d) = \{u= \sum_\omega \cos(2\pi \omega x) \hat u_\omega, \sum_k (1 + |\omega|^k ) |\hat u_k|< \infty\}
\end{align}

\subsubsection{Large $N$ limit}
Consider a two layer neural network
\begin{align}
	u_N = \frac{1}{N} \sum_{i=1}^N a_i \sigma(b_i x +c_i)
\end{align}
In the $N \gg 1$ limit, you can think of $\frac{1}{N} \sum_{i=1}^N \approx \int$, so this becomes
\begin{align}
	\int a\sigma(b x + c) \mu(da, db, dc)
\end{align}













































