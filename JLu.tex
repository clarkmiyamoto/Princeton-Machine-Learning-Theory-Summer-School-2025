In these set of lectures, we'll go over how to design machine learning algorithms to solve problems arising from PDEs.

\section{August 12th (1st session)}
Consider a PDE defined on the domain $\Omega \subset \mathbb R^d$. For example let's consider the driven heat equation.
\begin{align}
	-\Delta u &= f & \text{Inside } \Omega\\
	\partial_n u  &= 0 & \text{On } \partial \Omega
\end{align}
where $\Delta = \frac{d}{2} = - \partial^2$ is the laplacian.

We can massage this into a minimization problem, where you attempt to minimize the residual. This is called the \textbf{strong form}
\begin{align}
	\inf_u \int_\Omega |\Delta u + f|^2 
\end{align}
This technique has origins in mathematics long ago, however today, we ansatz $u=u_\theta$ using a neural network, and modify $\theta$ s.t. you minimize that quantity. These are known as \textbf{PINNs (Physics Informed Neural Networks)}.

Another method is to use the \textbf{variational form}
\begin{align}
	\inf_u \frac{1}{2} \int (|\nabla u|^2 - u f) ~dx 
\end{align}
You can convince yourself that this is correct by taking the variational derivative w.r.t. $u$, and see the minimizers is of the form of the PDE.

Another method is to use the \textbf{weak form}, where we expect this
\begin{align}
	\forall v~ \int v (-\Delta u \cdot f) = 0 \iff \int_\Omega \nabla v \cdot \nabla u - v f = 0
\end{align}

\begin{problem}
	To demonstrate your understanding, show the formulation of the strong, variational, and weak form in linear algebra. I.e. you want to solve $A x = b$, reformulate this into the various forms. 
\end{problem}

When realizing these problem on a computers, $u$ is a high-dimensional (or infinite dimensional) quantity, obviously this becomes a problem...

\subsection{Finite Dimensional Approximation (In Function Space)}
Consider decomposing $u$ into a basis $\{\varphi_i\}_i$
\begin{align}
	u(x) = \sum_{i=1}^N c_i \varphi_i(x)
\end{align}
Now it is your job to find $c_i$'s s.t. you minimize the variational form. So let's plug it in
\begin{align}
	\min_{\{c_i\}} \left[\frac{1}{2} \sum_{ij} \left( c_i c_j \int \nabla \varphi_i \cdot \nabla \varphi_j  \right)- \sum_i \left( c_i \int f \varphi_i dx \right) \right]
\end{align}
This is now just a quadratic minimization problem. Nice.\\
\\
However, this problem suffers from the curse of dimensionality. The number of coefficients grows exponential in the dimension. To see this, notice when you descrteize the domain $\Omega$, the number of grid points grows as $\text{(\# points)} = (L/\epsilon)^d$.\\
\\
So solving PDEs in high-dimension (in domain space) is an interesting problems. And it seems that neural networks can defeat the curse of dimensionality, so perhaps we need to combine the two!

\subsection{What can Neural Networks do for PDEs?}
So now that we've motived that neural networks can help us in PDEs, let's go over a few applications
\begin{enumerate}
	\item PINN: Using neural networks ansatz for solving PDEs. We've outlined this problem above.
	\item Operator learning: Your use the neural networks to map a function to another function (thus it learns an operator). I.e. this can be a time-evolution kernel. The difficulty lies in encoding an infinite dimensional space into a finite dimensional space.
	\item System identification:
\end{enumerate}


\subsection{Bounding of Error Due to Discretization}
Let $\mathcal X$ be some space over functions. Your true function $u_* \in \mathcal X$. Now consider a finite-dimensional approximation of the function space $u_N \in \mathcal X_N$, where we assume $\mathcal X_N \subset \mathcal X$ (this may not hold depending on the boundary condition you set). Note that your true solution $u_*$, may not necessarily be in $\mathcal X_N$. This yields a trivial bound on the residual
\begin{align}
	||u_* - u_N||_{x} \geq \underbrace{\inf_{u \in \mathcal X_n} ||u_* - u||_x}_{\text{Approximation error}}
\end{align}
Going back to the forced heat equation in the variational form, we want to minimze
\begin{align}
	\Sigma(u) & = \frac{1}{2} \int |\nabla u |^2 dx - \int u f dx\\
	\inf_{u \in \mathcal X} \Sigma(u) = \Sigma(u_*)
\end{align}
From the bound, we know that $\Sigma(u_N)  - \Sigma(u_*)\geq  0$. Plugging this in
\begin{align}
	\Sigma(u_N) - \Sigma(u_*) & = \frac{1}{2} \int( |\nabla u_N|^2 - |\nabla u_*|^2 ) dx + \int (u_N - u_*) \Delta u_* dx\\
	& = \frac{1}{2} \int |\nabla u_N - \nabla u_*|^2 dx
\end{align}
If $\Omega$ is bounded, then
\begin{align}
	\underbrace{\int |u_N - u_*|^2}_{||u_* - u_N||^2_x} dx < C \underbrace{\int |\nabla u_N - \nabla u_*|^2 dx}_{\sim (\Sigma(u_N) - \Sigma(u_*))}
\end{align}
And if you make an additional assumption that $\Omega$ is bounded AND convex, then you know that $C$ doesn't depend on $\text{dim}(\Omega)$. This gives us a pretty bound
\begin{align}
	\Sigma(u_N) - \Sigma(u_*) \geq  C || u_* - u_N||^2_x
\end{align}\
You can find another bound
\begin{align}
	\Sigma(u_N) - \Sigma(u_*) & = \inf_{u \in \mathcal X_N} \Sigma(u) - \Sigma(u_*)\\
	& = \inf_{u \in \mathcal X_n} \frac{1}{2} \int |\nabla u - \nabla u_*|^2\\
	& \leq \inf_{u \in \mathcal X_N} \frac{1}{2} || u - u_* ||^2_x
\end{align}
Putting this together, you get a very nice principled bound on the variational formulation of the problem.
\begin{align}
	\boxed{C || u_* - u_N||^2_x \leq \Sigma(u_N) - \Sigma(u_*) \leq \inf_{u \in \mathcal X_N} \frac{1}{2} || u_* - u ||^2_x}
\end{align}


\section{August 12th (2nd session)}
Recall that given a PDE, we have some energy functional $\Sigma(u)$ which has a minimizer $u_* \in \mathcal X$. We then ask the question, given a discretization of the function space $\text{argmin}_{u \in \mathcal X_N} \Sigma(u) = u_N \in \mathcal X_N \subset \mathcal X$, how does your approximated $u_N$ compare to the true answer $u_*$
\begin{align}
	\inf_{u \in \mathcal X_N}||u-u_*||_X
\end{align}
Consider the set of functions $H^k(\Omega) = \{u : \int |\partial_{i_1} ... \partial_{i_d} u|^2 d x < \infty, \sum_j i_j = i, i \leq k\}$

\begin{theorem}
	[Bramble-Hilbert Lemma] Let $u$ be a function defined on the domain $\Omega$, and $v \in P_k(\Omega)$ ($k$'th order polynomials defined on domain $\Omega$). Under "mild" assumptions...
	\begin{align}
		\inf_{v \in P_k(\Omega)} \int_\Omega |u^{(i)} - v^{(j)}|^2 dx \leq C (\text{diameter}(\Omega))^{m-k} \int_\Omega |u^{(m)}|^2 dx
	\end{align}
	where $u^{(i)}$ is the $i$'th order derivative, $(m)$ is the highest finite order derivative.\\
	\\
	As a result (which is not apparent to me a priori), the number of degrees of freedom scales $\sim h^{-d/s}$, where $h$ is the mesh size, $d$ is the dimension of the domain space $\Omega$, and $s$ are regularity conditions. So reading into it--- higher dimensional PDEs end up having "simpler" curves.
\end{theorem}

\begin{theorem}
	[Barron 1993] Consider the Barron function space 
	\begin{align}
		B(\mathbb R^d) = \Big\{u:  \mathbb R^d \to \mathbb R \Big | \int_{\mathbb R^d} |\omega | | \hat u (\omega) | d\omega < \infty\Big\}
	\end{align}
	where $\hat u$ is the Fourier transform of $u$. 
	\begin{align}
		H^1(\mathbb R^d) & = \{u : \mathbb R^d \to \mathbb R | \int |u|^2 + |\nabla u|^2 dx < \infty\}\\
		& = ... \int |\hat u (\omega)|^2 + |\omega|^2 \hat u (\omega)|^2 < \infty
	\end{align}
	The 2nd equality holds by Parseval's theorem, which states the $L_2$ norm is invariant under FT Transform.
	\\
	\\
	Then $\forall N$, there exists 
	\begin{align}
		u_N(x) = \sum_{i=1}^N a_i \sigma(b_i x + c_i)
	\end{align} s.t. the approximation error is bounded
	\begin{align}
		\int |u - u_N|^2 \mu(dx) \leq C \frac{||u||^2_{B(\mathbb R^d)}}{N}
	\end{align}
	\begin{sidework}
		To me this is kinda surprising because $N$ is the number of basis function which you use to approximate $u$. You'd imagine that as you increase the number of basis functions that such functions "over fit", and start to have high residuals... But I might be thinking of this wrong.
	\end{sidework}	
\end{theorem}

\begin{lemma}
	Let $f \in$ convex hull of a set $C$ in $X$ (Hilber space), with $||g|| \leq b$. You find that $\forall g \in G$, then for any $n \geq 1$ every $c > b^2 - ||f||^2$ there exists lies in the convex hull of $G$, s.t.
	\begin{align}
		||f-f_n||^2 \leq \frac{c}{n}
	\end{align}
	Proof: Let $||f - f^*|| \leq \delta / n$ where $f^*$ in the convex hull of $G$. Now do a basis decomposition
	\begin{align}
		f^*= \sum_{i=1}^M \nu_i g_{il}^*, ~~~~~\text{s.t.}~ \gamma_i \in [0,1], \sum_i \gamma_i =1
	\end{align}Randomly sample $g_1,..., g_n \sim \{g_j\}j$ with probability. This means 
	\begin{align}
		f_n= \frac{1}{n} \sum_{i}g_i
	\end{align}
\end{lemma}

Ok let's consider a screened poisson equation
\begin{align}
	(-\Delta + \gamma^2) u & = f & \text{Defined on } \mathbb R^d
\end{align}
Just by inspection, if $f$ is defined to the $k$ derivative, then we'd imagine that $u$ should be well defined for the $k+2$ derivatives. The LHS has two more derivatives. This implies $||u||_{H^{k+2}(\mathbb R^d)} \leq ||f||_{H^k(\mathbb R^d)}$.

\subsection{Cousins of Barron Spaces}
\subsubsection{Compact Spaces}
Consider a domain $\Omega = [0,1]^d$, you can define a barron space 
\begin{align}
	B^k([0,1]^d) = \{u= \sum_\omega \cos(2\pi \omega x) \hat u_\omega, \sum_k (1 + |\omega|^k ) |\hat u_k|< \infty\}
\end{align}

\subsubsection{Large $N$ limit}
Consider a two layer neural network
\begin{align}
	u_N = \frac{1}{N} \sum_{i=1}^N a_i \sigma(b_i x +c_i)
\end{align}
In the $N \gg 1$ limit, you can think of $\frac{1}{N} \sum_{i=1}^N \approx \int$, so this becomes
\begin{align}
	\int a\sigma(b x + c) \mu(da, db, dc)
\end{align}

\section{August 13th}
Recall, we were solving the variational problem, which involves minimizing the energy functional. For example, the forced-heat equation $-\Delta u = f$ has the variational form
\begin{align}
	\Sigma(u) & =  \int_\Omega (\frac{1}{2} |\nabla u|^2 - u f)dx\\
	& = |\Omega|~ \mathbb E_{x \sim \text{Unif}(\Omega)} [|\nabla u(x) |^2 - uf|]\\
	& \approx \frac{|\Omega|}{m} \sum_{i=1}^m |\nabla u(x_i)|^2 - u(x_i)f(x_i) \equiv \Sigma_m(u) & \text{Monte Carlo sampling}
\end{align}
Then we get $u_{N,m} = \inf_{u \in \mathcal X_N} \Sigma_m (u)$. We now ask how far this monte carlo finite dimensional approximation is from the true answer $u_* \in \mathcal X$
\begin{align}
	\mathbb E|| u_{N,m} - u_*||^2_{X} & \leq \mathbb E \Sigma(u_{N,m}) - \Sigma(u_*)\\
	  \begin{split}  & = \mathbb E \Big[ \Sigma(u_{N,m} - \Sigma_m(u_{N,m}) + \Sigma_m(u_{N,m})  - \Sigma_m (u_N) \\  & + \Sigma_m(u_N) - \Sigma_m(u_N) + \Sigma(u_N) - \Sigma(u_*) \Big] \end{split} & (\text{Insert } \pm 0) \\
	& \leq \text{Approximation Error}
\end{align}
Lu then spends the rest of the lecture reformulating various problems into this variational form problem. These discussions were non-informative, so I didn't take detailed notes
\begin{enumerate}
	\item Green's Functions:
	\item Eigenvalue problem: He basically just shows you can find the wave function in quantum mechanics using a monte-carlo scheme, see any numerical physics textbook on this.
	\item Iterative schemes:
	\item Non-linear PDEs (i.e. Optimal Control / Hamilton-Jacobi Bellman equation): He discusses 
\end{enumerate}



\section{August 14th}
In the spirit of Jane Street being a sponsor of the event, Lu will go over Hamilton Jacobi Bellman equations! Recall the HJB equation
\begin{align}
	- \gamma V(x) + \min_{u \in \mathbb R^m} \{\mathcal L_u  + \ell (x,u)\}= 0\\
	\text{s.t. }dX_t = b(X_t) dt + \sqrt{2} \sigma(X_t) dW_t
\end{align}
where $\mathcal L_u$ is the infinitesimal generator of the stochastic process, $X_t \in \mathbb R^d, b(X_t) \in \mathbb R^d,$ and $\sigma(X_t) \in \mathbb R^{d\times d}$.

\begin{sidework}
	Lu goes over a cute review of stochastic differential equations.

	Consider an ODE $dX_t = b(X_t) dt$. Say, you want to ask what is the gradient of $Y_t = V(X_t)$, obviously it's $\nabla_x V \cdot b(X_t)$ (via chain rule). But you can also derive this by asking what's the total derviative of the quantity and expanding, but we assume the usual calculus thing $dt \to 0$, so higher order terms $\mathcal O(dt^2)$ are negligible.
	
	If you do this for a SDE, all you need to know is $\mathcal O(dW_t^2) = \mathcal O(dt)$, then take the total derviative again, and kill terms $\mathcal O(dt^{3/2})$ or higher, you'll recover Ito's formula
	\begin{align}
		dV(X_t) = \nabla_x V (X_t) (b(X_t) dt + \sqrt{2}\sigma(X_t) dW_t) + \Tr(\nabla_x^2 V \sigma(X_t) \sigma(X_t)^T ) dt
	\end{align}
	Very cute.
\end{sidework}
So let's see what happens when we compute $dV(X_t)$
\begin{align}
	dV(X_t) = (\Delta  + b\cdot \nabla) V(X_t) dt + \sqrt{2} \nabla V(X_t) dW_t
\end{align}
Now ask, what is $\mathbb E[dV(X_t)] = d\mathbb E[V(X_t)]$
\begin{align}
	d\mathbb E[V(X_t)] = (\Delta + b \cdot \nabla) \mathbb E[V(X_t)] dt
\end{align}
Let's define $\mathbb E[V(X_t)] \equiv u(x,t)$, then this starts to look like a heat equation!
\begin{align}
	\partial_t u = (\Delta + b \cdot \nabla) u	
\end{align}
Now let's just assert a boundary conditoin $u(t=T,x) = h(x)$, this translates to $u(0,x) = \mathbb E h(X_T)$.\\
\\
Ok now let's consider a situation where the state equation has access to a control line
\begin{align}
	dX_t = b(X_t, u_t)dt + \sqrt{2} \sigma(X_t, u_t) dW_t
\end{align}
And we want to minimize a cost $J$, that is we want to find the best control line $u$
\begin{align}
	J(x,u)&  = \mathbb E \left[ \int_0^\infty e^{-\gamma t} \ell(x_t, u_t) dt | X_0 = x \right]\\
	V(x) & = \inf_u J(x,u)
\end{align}
For simplicity, let's assume $u$ must be Markovian, that is $u_t = u(t,X_t)$. By 


































